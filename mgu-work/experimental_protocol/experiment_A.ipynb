{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hypothesis\n",
    "\n",
    "## Hypothesis introduction\n",
    "\n",
    "The main proposed hypothesis behind this work is related to automatic neural graph topology architecture based on end-to-end training by directly using the task objective function. Thus we propose the _MultiGatedUnit_ or shortly _MGU_  - a graph module, or more intuitively said a complex neural network layer, that encapsulates any kind of neural computational layer (linear, convolutional, etc.) and it is able to self-learn the optimal sequencing configuration for the encapsulated layer through self-gating mechanisms.  \n",
    "\n",
    "Through direct optimization experiments on multiple datasets we argue that the proposed architecture drastically eliminates the need for grid search approaches required to find the optimal architecture of a deep neural graph.  As mentioned, using just one end-to-end optimization process, the deep neural graph using _MGUs_ will learn a _layer-level_ unique individual module structure for all modules within the graph. \n",
    "\n",
    "As an indirect result of this hypothesis we also argue that finding the _potentially ideal_ architecture just by using one single end-to-end optimization process drastically reduce de carbon footprint of the whole model generation process by eliminating the need for multiple end-to-end optimizations.\n",
    "In terms of clear objectives for the experiments, our aim is to:\n",
    "\n",
    " - demonstrate that our _MGU_ based deep neural graph has similar or better performance with the best models resulted from a classic grid-search operation\n",
    " - compare and contrast the compute time and thus carbon footprint of the whole grid-search process compared with the proposed _MGU_ based deep neural graph optimization\n",
    "\n",
    "## Short related work\n",
    "In terms of potentially similar work our proposed _MGU_ is directly related to the general area of neural graph topology architecturing, a long researched subject and still a hot one. The area of neural graph topology architecture has a multitude of different directions and techniques such as evolutionary methods, reinforcement learning based network architecture search. \n",
    "\n",
    "The direction of evolutionary approaches is probably the longest lasting research area in this subject - from early work of Miller et al in 1989 <cite data-cite=\"miller1989designing\"> </cite>, Stanley et al in 2002 <cite data-cite=\"stanley2002evolving\"> </cite> up to recent work such as that of Real et al from Google Brain <cite data-cite=\"real2019regularized\"> </cite> and their successful architecture _AmoebaNet_ .\n",
    "    \n",
    "Related to reinforcement learning based NAS we will mention probably one of the most important work in this area by Zoph et al <cite data-cite=\"zoph2018learning\"> </cite> a research that most likely fuelled multiple teams to further deepen this direction.\n",
    "    \n",
    "Worth mentioning is the fact that both above mentioned directions are _intelligent_ approaches of reducing the search-space _crawling_ however both are nonetheless based on actual grid-searching.\n",
    "    \n",
    "Finally, our most closely related researches are those that focus on direct optimization of topologies based on the task objective function. Thus, our proposed research relates directly to the early and important work of  Hochreiter and Schmidhuber  <cite data-cite=\"hochreiter1997long\"> </cite>, the more recent work of Schmidhuber et al in the area of self-gating mechanisms <cite data-cite=\"srivastava2015highway\"> </cite> and even more recent work of Hu et al <cite data-cite=\"hu2019squeezeandexcitation\"> </cite> in the same direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "## Overview\n",
    "\n",
    "For the testing of our hypothesis we propose 3 different public datasets ranging from the simple MNIST then growing the  complexity to CIFAR and CERVIGRAM datasets. CERVIGRAM <cite data-cite=\"xu2015new\"> </cite> - a little known yet public dataset provided by NIH - is a collection of data for cervical cancer classification based on colposcopy images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The warm-up\n",
    "\n",
    "In order to quickly iterate our experimentation protocol we decided to start with MNIST due to its simplicity and thus the correlated simplicity of the potential deep neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CERVIGRAM\n",
    "\n",
    "This particular dataset, made public through the work of Xu et al <cite data-cite=\"xu2015new\"> </cite>, consists in a few hundreds of medical cases where the patients have been either suspect or have been diagnosed with various stages of cervical dysplasia. Although limited in the number of cases, the dataset is thorough in terms of cervical dysplasia medical diagnosis protocol such as:\n",
    " - each case is defined by at least 5 different colposcopies taken at different times\n",
    " - beside the acetic-acid colposcopies and their corresponding images a 6th image is taken using green lens as well as a 7th image is added to the case after applying iodine solution to the target colposcopy area.\n",
    " - the analysis is done individually on each cervigram as well as a whole. There are 4 different classes: normal cervix (no dysplasia), very early lesion type 1 that requires only monitoring, more advanced lesions type 2 and type 3 that require treatment and the final class of cervical cancer.\n",
    "    \n",
    "While the dataset aims to analyze each case individually and for each case there are 7 different colposcopy images (or cervigrams) as mentioned above, we decided to us only 5 images out of each case - namely the images that only have acetic acid applied on target cervix area during the colposcopy process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "For our experimentation we decided to augment the classic hypothesis testing approaches based on often-used metrics such as accuracy, recall, precision with two other metrics that have the main purpose of measuring and comparing the carbon footprint of the _MGU_ based topology search and optimization process with the one based on grid-search. \n",
    "\n",
    "## Time-to-solution\n",
    "\n",
    "The \"time-to-solution\" metric purpose is to measure the time it takes to obtain the _best-model_ both with a grid-search approach as well as based on the optimization process of the _MGU_ based deep neural graph. In order to have a realistic comparison of the _MGU_ approach with that of a hyper-parameter space search we opted to analyze the potential search-space and limit it to the lowest number of dimensions possible. As a result, our hyper-parameters search algorithm performs a minimal number of iterations within the grid-search process. Finally we compare this total time with the time it takes to train, evaluate and _self-analyze_ the _MGU_ deep neural graph.\n",
    "\n",
    "## Accuracy performance\n",
    "\n",
    "Accuracy is a metric that needs not a special introduction and in the context of the three proposed public datasets classification accuracy is the straightforward used metric. \n",
    "\n",
    "## FLOPs/energy usage evaluation\n",
    "\n",
    "The actual energy usage evaluation is directly correlated with the time-to-solution metric.For this particular metric we decided to train all the models with a single GPU sequentially and **monitor the GPU loading for each end-to-end optimization process**. The main idea behind this approach is that the energy consumption of various architectures can drastically vary based on the numerical computation parallelization. While the _MGU_ training process is done in a single end-to-end optimization procedure it is nevertheless residing on heavy parallel computation and thus loads the GPU compute engine to a higher degree than some classic architectures (such as sequential convolutional deep neural graph architectures).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and approach\n",
    "\n",
    "In terms of the  actual model we already prepared a draft version of the _MGU_ module in PyTorch as well as a Tensorflow one. While both versions will be published on GitHub, the Tensorflow version already has an extended set of features including the self-diagnosis presented in the next main section.\n",
    "\n",
    "The main building block of _MGU_ is the gate-activation block that is composed of a linear transformation and a squash function - _sigmoid_ - that will constrain the gate-activation block output values between 0 and 1. Intuitively, the gate-activation output acts like a _pass_ or _not pass_ signal for a feature processor of the _MGU_'s input . In other words, the linear transformation weights encapsulates whether a feature processor is a good one or not to be _employed_ in the computational module.\n",
    "\n",
    "The current state of our work is focused towards automatic generation of graph topologies using the deep neural network feature processors that are currently most used both in convolutional and feed-forward neural networks literature such as _Batch Normalization_ <cite data-cite=\"Ioffe2015\"> </cite>, _Layer Normalization_ <cite data-cite=\"ba2016layer\"> </cite> and residual connections <cite data-cite=\"He2016\"> </cite>. Each gate-activation block inputs two feature processors and computes the _pass_ / _no pass_ signal given the _MGU_'s input. Thus, it has the purpose of choosing between one a certain feature processing method or pathways within the graph.\n",
    "\n",
    "## MGU Model formalization\n",
    "\n",
    "In the following equations we will present formalization of the _MultiGatedUnit_ information pipeline. We will denote $L_x = f_{lyr}(x)$, where $f_{lyr}$ is the _MGU_ encapsulated transformation and $x$ is the input of the _MGU_. Also, the encapsulated activation is denoted by $A(z)$ and a linear transofrmation of the input is denoted by $t(x)$. Please note that in below equations each individual gate _replicates_ the linear transformation $t(x)$ - such as $t_{bnpp}(x)$ for the first gate below - of the encapsulated layer in order to obtain a gating tensor similar to that resulted from the actual encapsulated operations.\n",
    "\n",
    "The first gate - $g_{bnpp}$ - is responsible for choosing whether to use pre-activation or post-activation _Batch Normalization_.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  l_{bnpp}(x) = t_{bnpp}(x)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  g_{bnpp}(x) = \\sigma(l_{bnpp}(x))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  f_{bn1}(x)= \\text{BN}(\\text{A}(L_x))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "   f_{bn2}(x) = \\text{A}(\\text{BN}(L_x)) \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  f_{bnpp}(x) = g_{bnpp}(x) * f_{bn1}(x) +  (1-g_{bnpp}(x)) * f_{bn2}(x)\n",
    "\\end{equation}\n",
    "\n",
    "The second $g_{bnln}$ gate chooses the best pathway between _Batch Normalization_ or _Layer Normalization_.\n",
    "\n",
    "\\begin{equation}\n",
    "l_{bnln}(x) = t_{bnln}(x)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " g_{bnln}(x) = \\sigma(l_{bnln}(x))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " f_{ln}(x) = \\text{A}(\\text{LN}(L_x))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " f_{bnln}(x) = g_{bnln}(x) * f_{bnpp}(x) + (1-g_{bnln}(x)) * f_{ln}(x)\n",
    "\\end{equation}\n",
    "\n",
    "The third gate decides whether it is better to use any kind of feature normalization or not.\n",
    "\n",
    "\\begin{equation}\n",
    "l_{non}(x) = t_{non}(x)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "g_{non}(x) = \\sigma(l_{non}(x))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "f_{non}(x) = g_{non}(x) * f_{bnln}(x) + (1-g_{non}(x)) * \\text{A}(L_x)\n",
    "\\end{equation}\n",
    "\n",
    "The fourth gate is able to choose whether to use or not residual connections.\n",
    "\n",
    "\\begin{equation}\n",
    "l_{ron}(x) = t_{ron}(x)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "g_{ron}(x) = \\sigma(l_{ron}(x))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " f_{res}(x) = f_{non}(x) + t(x)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "f_{ron}(x) = g_{ron}(x) * f_{non}(x) + (1-g_{ron}(x)) * f_{res}(x)\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the last gate can decide to skip all the feature processors and use the transformed input. The output of the last gate represents also the output of the _MultiGatedUnit_\n",
    "\n",
    "\\begin{equation}\n",
    "l_{skip}(x) = t_{skip}(x)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "g_{skip}(x) = \\sigma(l_{skip}(x))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "o(x) = g_{skip}(x) * f_{ron}(x) + (1-g_{skip}(x)) * t(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MGU model code\n",
    "\n",
    "In the following section a short snippet from the  _MGU_ python package (Tensorflow version) is presented containing the _forward_ propagation method. Below is part of the MultiGatedUnit module - for full definition will be available soon in the GitHub repository, prior to the presentation of the final paper.\n",
    "First we start with the GatingUnit that is basically a learnable gate layer. The name of the class has been modified to avoid confusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy 1.19.2\n",
      "Tensorflow 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "print('Numpy', np.__version__)\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow\", tf.__version__)\n",
    "\n",
    "class GatingUnit_Snippet(tf.keras.layers.Layer):\n",
    "  # the call, build, __init__ methods are not shown here\n",
    "  def _forward(self, inputs, for_model=False):\n",
    "    tf_source = inputs[0]\n",
    "    tf_value1 = inputs[1]\n",
    "    tf_value2 = inputs[2]\n",
    "    \n",
    "    tf_gate_x = self.gate_activ(\n",
    "      self.gate_trans(\n",
    "        tf_source\n",
    "        ))\n",
    "    tf_out = tf_gate_x * tf_value1 + (1 - tf_gate_x) * tf_value2    \n",
    "    return tf_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the main class. Again the name of the class has been modified to `MultiGatedUnit_Snippet` to avoid confusions with the actual class `MultiGatedUnit` that will be available in next two weeks on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGatedUnit_Snippet(tf.keras.layers.Layer):\n",
    "  # the call, build, __init__ methods are not shown here\n",
    "  def _forward(self, inputs, for_model=False):\n",
    "    tf_bypass = self.bypass(inputs)\n",
    "    tf_x = self.layer(inputs)\n",
    "    tf_x_act = self.act_pre_bn(tf_x)\n",
    "    \n",
    "    tf_x_bn_act = self.act_post_bn(self.bn_pre_act(tf_x))\n",
    "    tf_x_act_bn = self.bn_pos_act(tf_x_act)\n",
    "    tf_x_act_ln = self.ln_pos_act(tf_x_act)\n",
    "    \n",
    "    tf_bpre_bpos = self.g_bpre_bpos([inputs, tf_x_bn_act, tf_x_act_bn])\n",
    "    tf_bn_ln = self.g_bn_ln([inputs, tf_bpre_bpos, tf_x_act_ln])\n",
    "    tf_norm_non = self.g_norm_non([inputs, tf_bn_ln, tf_x_act])\n",
    "    tf_processed = tf_norm_non\n",
    "\n",
    "    tf_processed_res = tf_processed + tf_bypass\n",
    "    tf_final_processed = self.g_residual([inputs, tf_processed, tf_processed_res])\n",
    "\n",
    "    tf_proc_noproc = self.g_proc_skip([inputs, tf_bypass, tf_final_processed])\n",
    "    return tf_proc_noproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental pipeline and reasoning\n",
    " \n",
    "In order to demonstrate the neural graph architecture self-learning capabilities of _MGU_, we defined a hyperparameter search space that is, based on our hypothesis, currently covered by our neural graph.\n",
    "\n",
    "As described by the equations and the code in the above code snippet, our hypothesis is that the _MGU_ can automatically choose for each encapsulated computational layer which kind of normalization to do - pre-activation _Batch Normalization_, post-activation _Batch Normalization_, _Layer Normalization_ or no normalization. The _MGU_ can automatically choose for each computational layer whether it's better ot not to use residual connections and finally can _decide_ if the whole sub-graph should be skipped.\n",
    "\n",
    "In a basic setup without _MGU_ there would be 8 grid search resulted models (4 normalizations x 2 residual types) that need to be trained. However, finding the best deep neural model for a particular task requires grid searching through the number of computational elements that are stacked. Let's suppose that the grid search contains $L$ possibilities for the number of computational elements. Thus, the grid search space grows to $8*L$ model architectures possibilities. But, as already specified, the _MGU_ possesses also a _skip_ mechanism that allows the information to flow through the computational element unprocessed. In other words, we can stack a maximum number of _MGU_ s - i.e. the maximum number of the $L$ possibilities - and the neural network will self-learn how many of them should be used.\n",
    "\n",
    "Therefore, _MGU_ _discards_ $8*L$ grid search resulted models and the search space that the _MGU_ currently covers can be defined as following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_mgu_search_space = {\n",
    "  'norm': ['bn_pre', 'bn_post', 'ln', 'no_norm'],\n",
    "  'residual': [True, False],\n",
    "  'nr_computational_elements' : [3, 7, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect that should be mentioned is that in a basic grid search approach, for each grid search combination the resulting computational element is copied a certain number of times. This architectural constraint implies a clear limitation to the class of hypotheses. Considering the possibility that the optimal hypothesis might need unique structures for each individual module throughout the whole deep neural graph, this ideal hypothesis will never be found. **_MGU_ efficiently addresses this limitation because each individual _MGU_ computational element has its unique topology**. \n",
    "\n",
    "It's worth mentioning again that the gating mechanism uses the _sigmoid_ function. Thus, each gate does not impose a hard _pass_ or _no pass_. This aspect implies that each computational element can use fractions of information from all feature processors, which is (almost) impossible in a standard grid search process.\n",
    "\n",
    "With our proposed approach we do not aim to self-learn and bypass the need for identifying hyper-parameters such as learning rates, weight initialization strategies, the number of units for a _Dense_ layer, or the number of filters for a _Convolutional_ layer etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The grid search\n",
    "\n",
    "The hypothesis was tested for each individual dataset, using convolutional neural networks. A single _MGU_ versus other 24 models (4 normalizations x 2 residual types x 3 possibilities for the layout of the neural network) were benchmarked. We did not do an extensive grid search because we concentrated our experimentation to benchmark a single _MGU_  with the models resulted from the search space that the _MGU_ covers.\n",
    "\n",
    "The parameters used in the experimentation, but not grid searched, are:\n",
    "```\n",
    " - activation : 'selu'\n",
    " - pre_redout : 'avg_pool'\n",
    " - end_dropout : 0.3\n",
    " - kernel_size : 3\n",
    " - start_filters\n",
    "    - MNIST: 16\n",
    "    - CIFAR: 32\n",
    "    - CERVIGRAM: 32\n",
    " - max_filters\n",
    "    - MNIST: 512\n",
    "    - CIFAR: 1024    \n",
    "    - CERVIGRAM: 512\n",
    "```\n",
    "\n",
    "In order to get a valid shape of the feature map resulted after the convolution before the average pooling, the number of computational elements should come together with: `max_strides` (the maximum nr of strides applied to a convolution), `nr_reductions` (the count of the first convolutional layers on which are applied the maximum nr of strides), `scale_filters` (the factor applied to the previous convolutional layer's number of filters), `min_layer_padding_same` (which is the first convolutional layer where the padding changes from `valid` to `same`). All these parameters are denoted as `layout` in the presented grid search. We chose 3 different layouts for each individual dataset. The layout is presented as a list of items which can be decoded as following: `nr_layers, nr_reductions, max_strides, min_layer_padding_same, scale_filters = layout`.\n",
    "\n",
    "The resulted grid search dictionaries for each dataset are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search nr options:\n",
      "  - MNIST - basic: 24 / MGU: 1\n",
      "  - CIFAR - basic: 24 / MGU: 1\n",
      "  - CERVI - basic: 24 / MGU: 1\n"
     ]
    }
   ],
   "source": [
    "grid_search_mnist_basic = {\n",
    " 'layout': [[2, 2, 4, np.inf, 2], [3, 3, 2, np.inf, 2], [4, 2, 2, np.inf, 2]],\n",
    " 'norm': ['bn_pre', 'bn_post', 'ln', 'no_norm'],\n",
    " 'residual': [True, False],\n",
    " 'activation': ['selu'], 'pre_readout': ['avg_pool'], 'end_dropout': [0.3], 'kernel_size': [3],\n",
    " 'start_filters': [16], 'max_filters': [512]\n",
    "} \n",
    "\n",
    "grid_search_mnist_mgu = {\n",
    " 'layout': [[4, 2, 2, np.inf, 2]],\n",
    " 'activation': ['selu'], 'pre_readout': ['avg_pool'], 'end_dropout': [0.3], 'kernel_size': [3],\n",
    " 'start_filters': [16], 'max_filters': [512]\n",
    "}\n",
    "\n",
    "grid_search_cifar_basic = {\n",
    " 'layout': [[15, 1, 2, 8, 1.32], [7, 2, 2, 5, 1.32], [10, 1, 2, 8, 1.32]],\n",
    " 'norm': ['bn_pre', 'bn_post', 'ln', 'no_norm'],\n",
    " 'residual': [True, False],\n",
    " 'activation': ['selu'], 'pre_readout': ['avg_pool'], 'end_dropout': [0.3], 'kernel_size': [3],\n",
    " 'start_filters': [32], 'max_filters': [1024]\n",
    "}\n",
    "\n",
    "grid_search_cifar_mgu = {\n",
    " 'layout': [[15, 1, 2, 8, 1.32]],\n",
    " 'activation': ['selu'], 'pre_readout': ['avg_pool'], 'end_dropout': [0.3], 'kernel_size': [3],\n",
    " 'start_filters': [32], 'max_filters': [1024]\n",
    "}\n",
    "\n",
    "grid_search_cervigram_basic = {\n",
    " 'layout': [[8, 6, 2, np.inf, 1.32], [11, 5, 2, np.inf, 1.24], [17, 4, 2, np.inf, 1.14]],\n",
    " 'norm': ['bn_pre', 'bn_post', 'ln', 'no_norm'],\n",
    " 'residual': [True, False],\n",
    " 'activation': ['selu'], 'pre_readout': ['avg_pool'], 'end_dropout': [0.3], 'kernel_size': [3],\n",
    " 'start_filters': [32], 'max_filters': [512]\n",
    "}\n",
    "\n",
    "grid_search_cervigram_mgu = {\n",
    " 'layout': [[17, 4, 2, np.inf, 1.14]],\n",
    " 'activation': ['selu'], 'pre_readout': ['avg_pool'], 'end_dropout': [0.3], 'kernel_size': [3],\n",
    " 'start_filters': [32], 'max_filters': [512]\n",
    "}\n",
    "\n",
    "def nr_combinations(dct_grid):\n",
    "    n = 1\n",
    "    for k,v in dct_grid.items():\n",
    "        n*=len(v)\n",
    "    return n\n",
    "\n",
    "print(\"Grid search nr options:\")\n",
    "print(\"  - MNIST - basic: {} / MGU: {}\".format(nr_combinations(grid_search_mnist_basic), nr_combinations(grid_search_mnist_mgu)))\n",
    "print(\"  - CIFAR - basic: {} / MGU: {}\".format(nr_combinations(grid_search_cifar_basic), nr_combinations(grid_search_cifar_mgu)))\n",
    "print(\"  - CERVI - basic: {} / MGU: {}\".format(nr_combinations(grid_search_cervigram_basic), nr_combinations(grid_search_cervigram_mgu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet that interprets each resulted architecture can be inspected below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(mgu, shape, nr_classes, **kwargs):\n",
    "  tf.keras.backend.clear_session()\n",
    "  layout = kwargs.get('layout')\n",
    "  model_name = kwargs.get('model_name', '')\n",
    "  norm = kwargs.get('norm')\n",
    "  residual = kwargs.get('residual')\n",
    "  activation = kwargs.get('activation')\n",
    "  pre_readout = kwargs.get('pre_readout')\n",
    "  end_dropout = kwargs.get('end_dropout')\n",
    "  start_filters = kwargs.get('start_filters')\n",
    "  max_filters = kwargs.get('max_filters')\n",
    "  kernel_size = kwargs.get('kernel_size')\n",
    "\n",
    "  nr_layers, nr_reductions, max_strides,\\\n",
    "    min_layer_padding_same, scale_filters = layout\n",
    "\n",
    "  if mgu and 'MGU' not in model_name:\n",
    "    model_name = model_name + '_MGU'\n",
    "\n",
    "  conv_func = tf.keras.layers.Conv2D\n",
    "\n",
    "  tf_inp = tf.keras.layers.Input(shape)\n",
    "  tf_x = tf_inp\n",
    "\n",
    "  conv_layers = []\n",
    "  conv_layers_kwargs = []\n",
    "\n",
    "  for i in range(nr_layers):\n",
    "    filters = min(max_filters, int(start_filters * (scale_filters**i)))\n",
    "    strides = max_strides if i < nr_reductions else 1\n",
    "    padding = 'valid'\n",
    "    if i >= min_layer_padding_same-1:\n",
    "      padding = 'same'\n",
    "\n",
    "    conv_kwargs = dict(\n",
    "      filters = filters, kernel_size = kernel_size,\n",
    "      strides = strides, padding = padding\n",
    "    )\n",
    "    conv_layers.append(conv_func(**conv_kwargs))\n",
    "    conv_layers_kwargs.append(conv_kwargs)\n",
    "  #endfor\n",
    "\n",
    "  tf_residuals = [tf_x]\n",
    "  for i,layer in enumerate(conv_layers):\n",
    "\n",
    "    if mgu:\n",
    "      tf_x = MultiGatedUnit(layer, activation=activation)(tf_x)\n",
    "      continue\n",
    "\n",
    "    tf_x = layer(tf_x)\n",
    "    conv_kwargs = conv_layers_kwargs[i]\n",
    "\n",
    "    if norm == 'bn_pre':\n",
    "      tf_x = tf.keras.layers.BatchNormalization()(tf_x)\n",
    "      tf_x = tf.keras.layers.Activation(activation)(tf_x)\n",
    "\n",
    "    if norm == 'bn_post':\n",
    "      tf_x = tf.keras.layers.Activation(activation)(tf_x)\n",
    "      tf_x = tf.keras.layers.BatchNormalization()(tf_x)\n",
    "\n",
    "    if norm == 'ln':\n",
    "      tf_x = tf.keras.layers.Activation(activation)(tf_x)\n",
    "      tf_x = tf.keras.layers.LayerNormalization()(tf_x)\n",
    "\n",
    "    if norm == 'no_norm':\n",
    "      tf_x = tf.keras.layers.Activation(activation)(tf_x)\n",
    "\n",
    "    if residual and i >= 1:\n",
    "      lyr_transform = conv_func(**conv_kwargs)\n",
    "      tf_x = tf.keras.layers.add([tf_x, lyr_transform(tf_residuals[-1])])\n",
    "\n",
    "    tf_residuals.append(tf_x)\n",
    "  #endfor\n",
    "\n",
    "  if pre_readout == 'avg_pool':\n",
    "    tf_x = tf.keras.layers.GlobalAveragePooling2D()(tf_x)\n",
    "  else:\n",
    "    raise ValueError(\"Not implemented\")\n",
    "\n",
    "  tf_x = tf.keras.layers.Dropout(end_dropout)(tf_x)\n",
    "  tf_out = tf.keras.layers.Dense(nr_classes, activation='softmax')(tf_x)\n",
    "\n",
    "  model = tf.keras.models.Model(tf_inp, tf_out, name=model_name)\n",
    "\n",
    "  model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['acc'],\n",
    "  )\n",
    "\n",
    "  print(model.summary())\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization approach\n",
    "\n",
    "Each model was trained on GPU on a NVidia GeForce RTX 2080 Ti, using the same optimization approach - 100 epochs, 64 batch size for MNIST and CIFAR and 8 batch size for CERVIGRAM, _Nadam_ optimizer with an initial 0.01 learning rate that is time based decayed and early stopping on validation accuracy with patience of 10 epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress so far\n",
    "At the particular moment of research and experimentation process we managed to obtain results from the MNIST and CIFAR experiments, while implementing self-analysis features within the _MGU_ unit.\n",
    "\n",
    "## MNIST results and metrics analysis\n",
    "\n",
    "The results of the grid-searching experiment on MNIST can be inspected in the below table. The results are promising, as the _MGU_ based model surpasses all basic models in terms of validation accuracy - 99.32% for the _MGU_ vs 99.14% for the best non _MGU_. Moreover, the time to solution (`tts` column) is 876 seconds for the _MGU_ based model, whereas the basic grid search runs in approximately 8611 seconds - 9.84x speedup. This result drastically improves the energy consumption and the carbon footprint.\n",
    "\n",
    "_The model size column - `model_sz (K)` - is represented in thousands of parameters. Please note that the MGU model size is always **expanded** from the size of the original neural graph due to the addition of all the gate transformation matrices_.\n",
    "\n",
    "|    | model_name   | layout            | norm    |   residual | model_sz (K) |   tts |   train_acc |   val_acc |\n",
    "|---:|:-------------|:------------------|:--------|-----------:|-------------:|------:|------------:|----------:|\n",
    "|  0 | MNI_MGU_001  | [4, 2, 2, inf, 2] | N/A     |        N/A |       586.6  |   876 |       99.93 |     99.32 |\n",
    "|  1 | MNI_001      | [4, 2, 2, inf, 2] | ln      |          0 |        98.92 |   407 |       99.95 |     99.14 |\n",
    "|  2 | MNI_007      | [4, 2, 2, inf, 2] | bn_pre  |          0 |        99.4  |   248 |       99.76 |     99.13 |\n",
    "|  3 | MNI_003      | [4, 2, 2, inf, 2] | bn_post |          0 |        99.4  |   197 |       99.79 |     99.12 |\n",
    "|  4 | MNI_004      | [4, 2, 2, inf, 2] | bn_post |          1 |       196.39 |   363 |       99.73 |     99.07 |\n",
    "|  5 | MNI_005      | [4, 2, 2, inf, 2] | no_norm |          0 |        98.44 |   154 |       99.57 |     98.97 |\n",
    "|  6 | MNI_015      | [3, 3, 2, inf, 2] | bn_pre  |          0 |        24.39 |   404 |       99.7  |     98.97 |\n",
    "|  7 | MNI_008      | [4, 2, 2, inf, 2] | bn_pre  |          1 |       196.39 |   360 |       99.43 |     98.92 |\n",
    "|  8 | MNI_002      | [4, 2, 2, inf, 2] | ln      |          1 |       195.91 |   323 |       99.59 |     98.82 |\n",
    "|  9 | MNI_006      | [4, 2, 2, inf, 2] | no_norm |          1 |       195.43 |   298 |       99.59 |     98.82 |\n",
    "| 10 | MNI_011      | [3, 3, 2, inf, 2] | bn_post |          0 |        24.39 |   314 |       99.71 |     98.79 |\n",
    "| 11 | MNI_013      | [3, 3, 2, inf, 2] | no_norm |          0 |        23.95 |   332 |       99.73 |     98.77 |\n",
    "| 12 | MNI_009      | [3, 3, 2, inf, 2] | ln      |          0 |        24.17 |   433 |       99.87 |     98.77 |\n",
    "| 13 | MNI_012      | [3, 3, 2, inf, 2] | bn_post |          1 |        47.53 |   325 |       99.63 |     98.68 |\n",
    "| 14 | MNI_014      | [3, 3, 2, inf, 2] | no_norm |          1 |        47.08 |   305 |       99.34 |     98.52 |\n",
    "| 15 | MNI_010      | [3, 3, 2, inf, 2] | ln      |          1 |        47.31 |   300 |       99.4  |     98.45 |\n",
    "| 16 | MNI_016      | [3, 3, 2, inf, 2] | bn_pre  |          1 |        47.53 |   383 |       99.46 |     98.44 |\n",
    "| 17 | MNI_024      | [2, 2, 4, inf, 2] | bn_pre  |          1 |         9.96 |   494 |       93.61 |     93.12 |\n",
    "| 18 | MNI_020      | [2, 2, 4, inf, 2] | bn_post |          1 |         9.96 |   482 |       93.62 |     93.1  |\n",
    "| 19 | MNI_018      | [2, 2, 4, inf, 2] | ln      |          1 |         9.87 |   597 |       93.32 |     93.06 |\n",
    "| 20 | MNI_022      | [2, 2, 4, inf, 2] | no_norm |          1 |         9.77 |   338 |       93.18 |     92.99 |\n",
    "| 21 | MNI_019      | [2, 2, 4, inf, 2] | bn_post |          0 |         5.32 |   349 |       92.88 |     92.73 |\n",
    "| 22 | MNI_021      | [2, 2, 4, inf, 2] | no_norm |          0 |         5.13 |   437 |       92.61 |     92.61 |\n",
    "| 23 | MNI_023      | [2, 2, 4, inf, 2] | bn_pre  |          0 |         5.32 |   339 |       92.24 |     92.05 |\n",
    "| 24 | MNI_017      | [2, 2, 4, inf, 2] | ln      |          0 |         5.23 |   430 |       91.76 |     91.61 |\n",
    "\n",
    "## MGU self-diagnosis output\n",
    "\n",
    "One of the most important features that we managed to add at this time in the _MGU_ is a simple **self-explain-ability** method that enables the analysis of the gates within a _MGU_ module and presents the most-likely feature data-flow. For this particular feature we perform forward-passes of data throughout our model\n",
    "\n",
    "```\n",
    "Self-explainability of MGU layers in 6 layer graph\n",
    "...\n",
    "  Layer: MGU_conv2d_1 self-explainability analysis based on (12, 12, 16) input\n",
    "...\n",
    "    Analysing 'Gate3 - any norm or no norming at all'\n",
    "      Gate rule: `gate * Norming  +  (1 - gate) * No-norming`\n",
    "      Gate mean: 0.75, median: 0.87, min/max: 0.01/1.00\n",
    "      Gate opened for: Norming\n",
    "...\n",
    "    Analysing 'Gate5 - direct linear bypass or processed'\n",
    "      Gate rule: `gate * Bypass  +  (1 - gate) * Processed`\n",
    "      Gate mean: 0.32, median: 0.20, min/max: 0.00/0.99\n",
    "      Gate opened for: Processed\n",
    "```\n",
    "\n",
    "From the above partial explain-ability log output we see that for each individual gate the _MGU_ gives a _hint_ regarding the most likely path that the data flows throughout the graph. For example for the second _MGU_ `MGU_conv2d_1` the 5th gate blocks non-processed information from the bypass transformation while allowing the processed information to be passed forward - the actual activation distribution of the gate unit can be observed in below picture.\n",
    "\n",
    "![Gate unit activations](img_proc_vs_bypass.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 results and metrics analysis\n",
    "\n",
    "The results of the grid-searching experiment on CIFAR can be inspected in the below table. The results are also promising, as the _MGU_ based model achieves comparable validation accuracy with the best non _MGU_ model. The running time (`tts` column) for the _MGU_ based model is 1089 seconds, whereas the basic grid search runs in approximately 13450 seconds - 12.35x speedup. This result drastically improves the energy consumption and the carbon footprint.\n",
    "\n",
    "The model size column - `model_sz (M)` - is represented in millions of parameters.\n",
    "\n",
    "|    | model_name   | layout              | norm    |   residual | model_sz (M) |   tts |   train_acc |   val_acc |\n",
    "|---:|:-------------|:--------------------|:--------|-----------:|-------------:|------:|------------:|----------:|\n",
    "|  0 | CIF_016      | [10, 1, 2, 8, 1.32] | bn_pre  |          0 |         2.41 |   494 |       88.72 |     74.38 |\n",
    "|  1 | CIF_008      | [15, 1, 2, 8, 1.32] | bn_pre  |          0 |        30.51 |  1456 |       93.06 |     74.13 |\n",
    "|  2 | CIF_MGU_002  | [7, 2, 2, 5, 1.32]  | N/A     |        N/A |         3.09 |  1089 |       95.02 |     73.96 |\n",
    "|  3 | CIF_006      | [15, 1, 2, 8, 1.32] | bn_post |          0 |        30.51 |   910 |       92.71 |     73.83 |\n",
    "|  4 | CIF_014      | [10, 1, 2, 8, 1.32] | bn_post |          0 |         2.41 |   347 |       92.2  |     73.2  |\n",
    "|  5 | CIF_024      | [7, 2, 2, 5, 1.32]  | bn_pre  |          0 |         0.44 |   386 |       87.55 |     71.18 |\n",
    "|  6 | CIF_022      | [7, 2, 2, 5, 1.32]  | bn_post |          0 |         0.44 |   244 |       86.52 |     70.24 |\n",
    "|  7 | CIF_011      | [10, 1, 2, 8, 1.32] | ln      |          1 |         4.81 |   689 |       82.37 |     68.96 |\n",
    "|  8 | CIF_021      | [7, 2, 2, 5, 1.32]  | bn_post |          1 |         0.88 |   319 |       81.82 |     68.74 |\n",
    "|  9 | CIF_020      | [7, 2, 2, 5, 1.32]  | ln      |          0 |         0.44 |   251 |       84.79 |     68.68 |\n",
    "| 10 | CIF_018      | [7, 2, 2, 5, 1.32]  | no_norm |          0 |         0.44 |   235 |       86.51 |     67.78 |\n",
    "| 11 | CIF_019      | [7, 2, 2, 5, 1.32]  | ln      |          1 |         0.88 |   548 |       91.74 |     67.34 |\n",
    "| 12 | CIF_013      | [10, 1, 2, 8, 1.32] | bn_post |          1 |         4.81 |   699 |       85.83 |     66.92 |\n",
    "| 13 | CIF_015      | [10, 1, 2, 8, 1.32] | bn_pre  |          1 |         4.81 |   715 |       75.75 |     66.08 |\n",
    "| 14 | CIF_023      | [7, 2, 2, 5, 1.32]  | bn_pre  |          1 |         0.88 |   344 |       73    |     65.29 |\n",
    "| 15 | CIF_010      | [10, 1, 2, 8, 1.32] | no_norm |          0 |         2.4  |   258 |       83.32 |     61.09 |\n",
    "| 16 | CIF_017      | [7, 2, 2, 5, 1.32]  | no_norm |          1 |         0.88 |   270 |       49.24 |     47.91 |\n",
    "| 17 | CIF_009      | [10, 1, 2, 8, 1.32] | no_norm |          1 |         4.8  |   213 |       47.26 |     44.85 |\n",
    "| 18 | CIF_007      | [15, 1, 2, 8, 1.32] | bn_pre  |          1 |        60.99 |  1579 |       46.4  |     43.47 |\n",
    "| 19 | CIF_MGU_001  | [15, 1, 2, 8, 1.32] | N/A     |        N/A |       182.93 |  2115 |       34.98 |     33.95 |\n",
    "| 20 | CIF_005      | [15, 1, 2, 8, 1.32] | bn_post |          1 |        60.99 |   777 |       26.25 |     25.82 |\n",
    "| 21 | CIF_003      | [15, 1, 2, 8, 1.32] | ln      |          1 |        60.98 |   733 |       20.14 |     20.2  |\n",
    "| 22 | CIF_001      | [15, 1, 2, 8, 1.32] | no_norm |          1 |        60.96 |  1058 |       17.67 |     17.71 |\n",
    "| 23 | CIF_002      | [15, 1, 2, 8, 1.32] | no_norm |          0 |        30.49 |   366 |       10    |     10    |\n",
    "| 24 | CIF_012      | [10, 1, 2, 8, 1.32] | ln      |          0 |         2.41 |   187 |       10    |     10    |\n",
    "| 25 | CIF_004      | [15, 1, 2, 8, 1.32] | ln      |          0 |        30.5  |   417 |       10    |     10    |\n",
    "\n",
    "\n",
    "The first _MGU_ based model - _CIF_MGU_001_ that used the 15 stacked computational modules which is the maximum number of computational modules in the chosen CIFAR-10 grid search. As a result, this model is over-parametrized and thus, its optimization diverges after the first epoch. The over-parametrization comes from the added gate transformations for _Convolutional_ layers. We train another _MGU_ based model that uses less computational elements - _CIF_MGU_002_ - and it behaves as expected.\n",
    "\n",
    "![](img_cifar10_mgu_fail.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next steps\n",
    "\n",
    "So far we managed to obtain a stable version of the _MGU_ module including the self-explanation feature for Tensorflow. We also prepared a lighter version of _MGU_ for PyTorch framework. We also experimented and evaluated the behavior of the MGU on two initial public datasets and clearly identified the need to have more efficient gating transformations. Beside the public dataset experimental work we already employed the _MGU_ on private production-grade systems.\n",
    "\n",
    "## CERVIGRAM experiment\n",
    "More work is required to obtain consistent evidence regarding the application of our proposed innovation. Currently experimentation is underway with CERVIGRAM dataset.\n",
    "\n",
    "## Predictive analytics experiment\n",
    "Moreover, we will prepare for the final paper experiments proof that _MGU_ can be applied to other _layer_ types such as _linear_ layers in predictive analytics experiments. The application of _MGU_ on private datasets already has yielded good results and we are preparing the report on those experiments as well. \n",
    "\n",
    "## Over-parametrization\n",
    "Moreover, we will address the over parametrization problem efficiently either with heuristical approaches such as using depth-wise convolutions transformations for convolutional gates or with low-dimensionality embedding generation similar to <cite data-cite=\"hu2019squeezeandexcitation\"> </cite>. Last but not least we will explore further feature processors possibilities to include in the _MultiGatedUnit_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
